{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9dfcf7ea",
   "metadata": {},
   "source": [
    "Code in order to transform our dataset from grayscale images of peoples faces into feature maps which we will train a classifier on. Only needs to be run once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93a6359d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ecs179/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8cc62bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "\n",
    "# Indices from the mediapipe_landmarks_detection gist and common docs\n",
    "FACE_LEFT_CHEEK = 234\n",
    "FACE_RIGHT_CHEEK = 454\n",
    "\n",
    "MOUTH_LEFT = 61\n",
    "MOUTH_RIGHT = 291\n",
    "MOUTH_TOP_INNER = 13\n",
    "MOUTH_BOTTOM_INNER = 14\n",
    "NOSE_TIP = 1\n",
    "\n",
    "# Eyes\n",
    "RIGHT_EYE_OUTER = 33\n",
    "RIGHT_EYE_INNER = 133\n",
    "RIGHT_EYE_TOP = 159\n",
    "RIGHT_EYE_BOTTOM = 145\n",
    "\n",
    "LEFT_EYE_OUTER = 263\n",
    "LEFT_EYE_INNER = 362\n",
    "LEFT_EYE_TOP = 386\n",
    "LEFT_EYE_BOTTOM = 374\n",
    "\n",
    "# Eyebrow groups from the gist you saw\n",
    "LEFT_EYE_INDICES = [362, 382, 381, 380, 374, 373, 390, 249, 263, 466,\n",
    "                    388, 387, 386, 385, 384, 398]\n",
    "RIGHT_EYE_INDICES = [33, 7, 163, 144, 145, 153, 154, 155, 133, 173,\n",
    "                     157, 158, 159, 160, 161, 246]\n",
    "\n",
    "LEFT_EYEBROW_INDICES = [336, 296, 334, 293, 300, 276, 283, 282, 295, 285]\n",
    "RIGHT_EYEBROW_INDICES = [70, 63, 105, 66, 107, 55, 65, 52, 53, 46]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f686d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_feature_array(feature_dict):\n",
    "    \"\"\"\n",
    "    Takes a dict {name: value} and returns:\n",
    "      - values as np.float32 array\n",
    "      - list of names in matching order\n",
    "    \"\"\"\n",
    "    names = list(feature_dict.keys())\n",
    "    values = np.array(list(feature_dict.values()), dtype=np.float32)\n",
    "    return values, names\n",
    "\n",
    "\n",
    "def _dist(p1, p2):\n",
    "    return np.linalg.norm(p1 - p2)\n",
    "\n",
    "\n",
    "def _slope(p_from, p_to):\n",
    "    dx = p_to[0] - p_from[0]\n",
    "    dy = p_to[1] - p_from[1]\n",
    "    return dy / (dx + 1e-6)\n",
    "\n",
    "\n",
    "def _signed_point_line_distance_2d(p, a, b):\n",
    "    \"\"\"\n",
    "    Signed distance from point p to the line through a and b in xy.\n",
    "    Uses 2D cross product for sign.\n",
    "    \"\"\"\n",
    "    p2 = p[:2]\n",
    "    a2 = a[:2]\n",
    "    b2 = b[:2]\n",
    "    v = b2 - a2\n",
    "    w = p2 - a2\n",
    "    cross = v[0] * w[1] - v[1] * w[0]\n",
    "    denom = np.linalg.norm(v) + 1e-6\n",
    "    return cross / denom\n",
    "\n",
    "\n",
    "def _get_inner_outer_brow_points(pts, indices, side):\n",
    "    \"\"\"\n",
    "    side: 'left' or 'right' from subject perspective.\n",
    "    Uses x coordinate to choose inner vs outer point.\n",
    "    \"\"\"\n",
    "    brow_pts = pts[indices]          # shape (N, 3)\n",
    "    xs = brow_pts[:, 0]\n",
    "\n",
    "    if side == \"left\":\n",
    "        # Subject left brow is on viewer right\n",
    "        # inner is closer to center (smaller x), outer larger x\n",
    "        inner_local = int(xs.argmin())\n",
    "        outer_local = int(xs.argmax())\n",
    "    else:\n",
    "        # Subject right brow is on viewer left\n",
    "        # inner is closer to center (larger x), outer smaller x\n",
    "        inner_local = int(xs.argmax())\n",
    "        outer_local = int(xs.argmin())\n",
    "\n",
    "    inner_pt = brow_pts[inner_local]\n",
    "    outer_pt = brow_pts[outer_local]\n",
    "    return inner_pt, outer_pt\n",
    "\n",
    "\n",
    "def extract_features(landmarks):\n",
    "    \"\"\"\n",
    "    landmarks: list of 468 mediapipe landmarks (with .x, .y, .z)\n",
    "    Returns:\n",
    "        features: 1D np.array of floats\n",
    "        feature_names: list of strings in matching order\n",
    "    \"\"\"\n",
    "    pts = np.array([[lm.x, lm.y, lm.z] for lm in landmarks])\n",
    "\n",
    "    # basic reference distances\n",
    "    left_cheek = pts[FACE_LEFT_CHEEK]\n",
    "    right_cheek = pts[FACE_RIGHT_CHEEK]\n",
    "    face_width = _dist(left_cheek, right_cheek) + 1e-6\n",
    "\n",
    "    forehead = pts[10]\n",
    "    chin = pts[152]\n",
    "    face_height = _dist(forehead, chin) + 1e-6\n",
    "\n",
    "    # mouth points\n",
    "    mouth_left = pts[MOUTH_LEFT]\n",
    "    mouth_right = pts[MOUTH_RIGHT]\n",
    "    mouth_top = pts[MOUTH_TOP_INNER]\n",
    "    mouth_bottom = pts[MOUTH_BOTTOM_INNER]\n",
    "    nose_tip = pts[NOSE_TIP]\n",
    "    mouth_center = 0.5 * (mouth_left + mouth_right)\n",
    "\n",
    "    # base mouth features\n",
    "    mouth_width = _dist(mouth_left, mouth_right) / face_width\n",
    "    mouth_height = _dist(mouth_top, mouth_bottom) / face_width\n",
    "    mouth_ar = mouth_height / (mouth_width + 1e-6)\n",
    "\n",
    "    mouth_corner_asym = (mouth_left[1] - mouth_right[1])\n",
    "    mouth_center_nose_dist = _dist(mouth_center, nose_tip) / face_height\n",
    "\n",
    "    slope_left = _slope(mouth_center, mouth_left)\n",
    "    slope_right = _slope(mouth_center, mouth_right)\n",
    "    slope_mean = 0.5 * (slope_left + slope_right)\n",
    "    slope_diff = slope_left - slope_right\n",
    "\n",
    "    # extra mouth geometry\n",
    "    lip_thickness_mid = _dist(mouth_top, mouth_bottom) / face_height\n",
    "\n",
    "    mouth_corner_top_left = _dist(mouth_left, mouth_top) / face_height\n",
    "    mouth_corner_top_right = _dist(mouth_right, mouth_top) / face_height\n",
    "    mouth_corner_bottom_left = _dist(mouth_left, mouth_bottom) / face_height\n",
    "    mouth_corner_bottom_right = _dist(mouth_right, mouth_bottom) / face_height\n",
    "\n",
    "    mouth_corner_nose_left = _dist(mouth_left, nose_tip) / face_height\n",
    "    mouth_corner_nose_right = _dist(mouth_right, nose_tip) / face_height\n",
    "\n",
    "    upper_lip_curvature = _signed_point_line_distance_2d(\n",
    "        mouth_top, mouth_left, mouth_right\n",
    "    ) / (face_height + 1e-6)\n",
    "    lower_lip_curvature = _signed_point_line_distance_2d(\n",
    "        mouth_bottom, mouth_left, mouth_right\n",
    "    ) / (face_height + 1e-6)\n",
    "\n",
    "    # eyes\n",
    "    left_eye_outer = pts[LEFT_EYE_OUTER]\n",
    "    left_eye_inner = pts[LEFT_EYE_INNER]\n",
    "    left_eye_top = pts[LEFT_EYE_TOP]\n",
    "    left_eye_bottom = pts[LEFT_EYE_BOTTOM]\n",
    "\n",
    "    right_eye_outer = pts[RIGHT_EYE_OUTER]\n",
    "    right_eye_inner = pts[RIGHT_EYE_INNER]\n",
    "    right_eye_top = pts[RIGHT_EYE_TOP]\n",
    "    right_eye_bottom = pts[RIGHT_EYE_BOTTOM]\n",
    "\n",
    "    left_eye_width = _dist(left_eye_outer, left_eye_inner) / face_width\n",
    "    left_eye_height = _dist(left_eye_top, left_eye_bottom) / face_width\n",
    "\n",
    "    right_eye_width = _dist(right_eye_outer, right_eye_inner) / face_width\n",
    "    right_eye_height = _dist(right_eye_top, right_eye_bottom) / face_width\n",
    "\n",
    "    left_ear = left_eye_height / (left_eye_width + 1e-6)\n",
    "    right_ear = right_eye_height / (right_eye_width + 1e-6)\n",
    "    mean_ear = 0.5 * (left_ear + right_ear)\n",
    "    ear_diff = left_ear - right_ear\n",
    "\n",
    "    # eye centers and cheek distances\n",
    "    left_eye_center = pts[LEFT_EYE_INDICES].mean(axis=0)\n",
    "    right_eye_center = pts[RIGHT_EYE_INDICES].mean(axis=0)\n",
    "\n",
    "    left_cheek_eye = _dist(left_cheek, left_eye_center) / face_width\n",
    "    right_cheek_eye = _dist(right_cheek, right_eye_center) / face_width\n",
    "    cheek_eye_mean = 0.5 * (left_cheek_eye + right_cheek_eye)\n",
    "    cheek_eye_diff = left_cheek_eye - right_cheek_eye\n",
    "\n",
    "    # eyebrow to eye distances (existing mean based)\n",
    "    left_eye_mean_y = pts[LEFT_EYE_INDICES][:, 1].mean()\n",
    "    right_eye_mean_y = pts[RIGHT_EYE_INDICES][:, 1].mean()\n",
    "\n",
    "    left_brow_mean_y = pts[LEFT_EYEBROW_INDICES][:, 1].mean()\n",
    "    right_brow_mean_y = pts[RIGHT_EYEBROW_INDICES][:, 1].mean()\n",
    "\n",
    "    left_brow_eye = (left_brow_mean_y - left_eye_mean_y) / (face_height + 1e-6)\n",
    "    right_brow_eye = (right_brow_mean_y - right_eye_mean_y) / (face_height + 1e-6)\n",
    "    brow_eye_mean = 0.5 * (left_brow_eye + right_brow_eye)\n",
    "    brow_eye_diff = left_brow_eye - right_brow_eye\n",
    "\n",
    "    # inner vs outer brow points and richer brow geometry\n",
    "    left_inner_brow, left_outer_brow = _get_inner_outer_brow_points(\n",
    "        pts, LEFT_EYEBROW_INDICES, side=\"left\"\n",
    "    )\n",
    "    right_inner_brow, right_outer_brow = _get_inner_outer_brow_points(\n",
    "        pts, RIGHT_EYEBROW_INDICES, side=\"right\"\n",
    "    )\n",
    "\n",
    "    inner_brow_dist = _dist(left_inner_brow, right_inner_brow) / face_width\n",
    "\n",
    "    left_brow_tilt = _slope(left_inner_brow, left_outer_brow)\n",
    "    right_brow_tilt = _slope(right_inner_brow, right_outer_brow)\n",
    "    brow_tilt_diff = left_brow_tilt - right_brow_tilt\n",
    "\n",
    "    left_brow_mid = pts[LEFT_EYEBROW_INDICES].mean(axis=0)\n",
    "    right_brow_mid = pts[RIGHT_EYEBROW_INDICES].mean(axis=0)\n",
    "\n",
    "    left_brow_curv = _signed_point_line_distance_2d(\n",
    "        left_brow_mid, left_inner_brow, left_outer_brow\n",
    "    ) / (face_height + 1e-6)\n",
    "    right_brow_curv = _signed_point_line_distance_2d(\n",
    "        right_brow_mid, right_inner_brow, right_outer_brow\n",
    "    ) / (face_height + 1e-6)\n",
    "    brow_curv_mean = 0.5 * (left_brow_curv + right_brow_curv)\n",
    "    brow_curv_diff = left_brow_curv - right_brow_curv\n",
    "\n",
    "    left_inner_brow_eye = (left_inner_brow[1] - left_eye_center[1]) / (face_height + 1e-6)\n",
    "    left_outer_brow_eye = (left_outer_brow[1] - left_eye_center[1]) / (face_height + 1e-6)\n",
    "    right_inner_brow_eye = (right_inner_brow[1] - right_eye_center[1]) / (face_height + 1e-6)\n",
    "    right_outer_brow_eye = (right_outer_brow[1] - right_eye_center[1]) / (face_height + 1e-6)\n",
    "\n",
    "    inner_brow_eye_mean = 0.5 * (left_inner_brow_eye + right_inner_brow_eye)\n",
    "    outer_brow_eye_mean = 0.5 * (left_outer_brow_eye + right_outer_brow_eye)\n",
    "    inner_outer_brow_eye_diff = inner_brow_eye_mean - outer_brow_eye_mean\n",
    "\n",
    "    # global geometry\n",
    "    face_aspect = face_height / face_width\n",
    "\n",
    "    v = right_cheek - left_cheek\n",
    "    head_tilt_angle = np.arctan2(v[1], v[0])\n",
    "\n",
    "    mouth_corner_asym_norm = mouth_corner_asym / (face_height + 1e-6)\n",
    "\n",
    "    head_tilt_sin = np.sin(head_tilt_angle)\n",
    "    head_tilt_cos = np.cos(head_tilt_angle)\n",
    "\n",
    "    # build dictionary of all features so names and values stay aligned\n",
    "    feature_dict = {\n",
    "        # mouth\n",
    "        \"mouth_width\": mouth_width,\n",
    "        \"mouth_height\": mouth_height,\n",
    "        \"mouth_ar\": mouth_ar,\n",
    "        \"mouth_corner_asym_norm\": mouth_corner_asym_norm,\n",
    "        \"mouth_center_nose_dist\": mouth_center_nose_dist,\n",
    "        \"mouth_slope_left\": slope_left,\n",
    "        \"mouth_slope_right\": slope_right,\n",
    "        \"mouth_slope_mean\": slope_mean,\n",
    "        \"mouth_slope_diff\": slope_diff,\n",
    "\n",
    "        # extra mouth\n",
    "        \"lip_thickness_mid\": lip_thickness_mid,\n",
    "        \"mouth_corner_top_left\": mouth_corner_top_left,\n",
    "        \"mouth_corner_top_right\": mouth_corner_top_right,\n",
    "        \"mouth_corner_bottom_left\": mouth_corner_bottom_left,\n",
    "        \"mouth_corner_bottom_right\": mouth_corner_bottom_right,\n",
    "        \"mouth_corner_nose_left\": mouth_corner_nose_left,\n",
    "        \"mouth_corner_nose_right\": mouth_corner_nose_right,\n",
    "        \"upper_lip_curvature\": upper_lip_curvature,\n",
    "        \"lower_lip_curvature\": lower_lip_curvature,\n",
    "\n",
    "        # eyes\n",
    "        \"left_eye_width\": left_eye_width,\n",
    "        \"left_eye_height\": left_eye_height,\n",
    "        \"right_eye_width\": right_eye_width,\n",
    "        \"right_eye_height\": right_eye_height,\n",
    "        \"left_ear\": left_ear,\n",
    "        \"right_ear\": right_ear,\n",
    "        \"mean_ear\": mean_ear,\n",
    "        \"ear_diff\": ear_diff,\n",
    "\n",
    "        # eye-cheek\n",
    "        \"left_cheek_eye\": left_cheek_eye,\n",
    "        \"right_cheek_eye\": right_cheek_eye,\n",
    "        \"cheek_eye_mean\": cheek_eye_mean,\n",
    "        \"cheek_eye_diff\": cheek_eye_diff,\n",
    "\n",
    "        # brows\n",
    "        \"left_brow_eye\": left_brow_eye,\n",
    "        \"right_brow_eye\": right_brow_eye,\n",
    "        \"brow_eye_mean\": brow_eye_mean,\n",
    "        \"brow_eye_diff\": brow_eye_diff,\n",
    "\n",
    "        # brow geometry\n",
    "        \"inner_brow_dist\": inner_brow_dist,\n",
    "        \"left_brow_tilt\": left_brow_tilt,\n",
    "        \"right_brow_tilt\": right_brow_tilt,\n",
    "        \"brow_tilt_diff\": brow_tilt_diff,\n",
    "        \"left_brow_curv\": left_brow_curv,\n",
    "        \"right_brow_curv\": right_brow_curv,\n",
    "        \"brow_curv_mean\": brow_curv_mean,\n",
    "        \"brow_curv_diff\": brow_curv_diff,\n",
    "        \"left_inner_brow_eye\": left_inner_brow_eye,\n",
    "        \"left_outer_brow_eye\": left_outer_brow_eye,\n",
    "        \"right_inner_brow_eye\": right_inner_brow_eye,\n",
    "        \"right_outer_brow_eye\": right_outer_brow_eye,\n",
    "        \"inner_brow_eye_mean\": inner_brow_eye_mean,\n",
    "        \"outer_brow_eye_mean\": outer_brow_eye_mean,\n",
    "        \"inner_outer_brow_eye_diff\": inner_outer_brow_eye_diff,\n",
    "\n",
    "        # global\n",
    "        \"face_aspect\": face_aspect,\n",
    "        \"head_tilt_angle\": head_tilt_angle,\n",
    "        \"head_tilt_sin\": head_tilt_sin,\n",
    "        \"head_tilt_cos\": head_tilt_cos,\n",
    "    }\n",
    "\n",
    "    features, feature_names = make_feature_array(feature_dict)\n",
    "    return features, feature_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "722a0bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_dataset_paths():\n",
    "    path = kagglehub.dataset_download(\"jonathanoheix/face-expression-recognition-dataset\")\n",
    "    train_dir = os.path.join(path, \"images\", \"train\")\n",
    "    val_dir = os.path.join(path, \"images\", \"validation\")\n",
    "    return train_dir, val_dir\n",
    "\n",
    "def iterate_images(root_dir):\n",
    "    class_names = sorted([\n",
    "        d for d in os.listdir(root_dir)\n",
    "        if os.path.isdir(os.path.join(root_dir, d))\n",
    "    ])\n",
    "\n",
    "    for label in class_names:\n",
    "        class_dir = os.path.join(root_dir, label)\n",
    "        for fname in os.listdir(class_dir):\n",
    "            if not fname.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n",
    "                continue\n",
    "            full_path = os.path.join(class_dir, fname)\n",
    "            yield full_path, label\n",
    "\n",
    "\n",
    "def preprocess_to_csv(root_dir, output_csv, img_size=48):\n",
    "    face_mesh = mp_face_mesh.FaceMesh(\n",
    "        static_image_mode=True,\n",
    "        max_num_faces=1,\n",
    "        refine_landmarks=True,\n",
    "        min_detection_confidence=0.5,\n",
    "    )\n",
    "\n",
    "    rows = []\n",
    "    header = None\n",
    "    count = 0\n",
    "\n",
    "    for img_path, label in iterate_images(root_dir):\n",
    "        img_gray = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if img_gray is None:\n",
    "            continue\n",
    "\n",
    "        img_gray = cv2.resize(img_gray, (img_size, img_size))\n",
    "        img_rgb = cv2.cvtColor(img_gray, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "        result = face_mesh.process(img_rgb)\n",
    "        if not result.multi_face_landmarks:\n",
    "            continue\n",
    "\n",
    "        landmarks = result.multi_face_landmarks[0].landmark\n",
    "\n",
    "        # extract both (features, feature_names)\n",
    "        feat_values, feat_names = extract_features(landmarks)\n",
    "\n",
    "        # Build header ONCE using descriptive names\n",
    "        if header is None:\n",
    "            header = feat_names + [\"label\"]\n",
    "\n",
    "        rows.append(feat_values.tolist() + [label])\n",
    "        count += 1\n",
    "\n",
    "        if count % 500 == 0:\n",
    "            print(f\"Processed {count} images from {root_dir}...\")\n",
    "\n",
    "    if not rows:\n",
    "        print(f\"No rows collected for {root_dir}, nothing to write.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Writing {len(rows)} rows to {output_csv}\")\n",
    "\n",
    "    with open(output_csv, \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(header)  \n",
    "        writer.writerows(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddc0f76c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1764916129.240997 3565066 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 90.5), renderer: Apple M1\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1764916129.259933 3568611 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1764916129.268225 3568615 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1764916129.293453 3568618 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 500 images from /Users/afifahhadi/.cache/kagglehub/datasets/jonathanoheix/face-expression-recognition-dataset/versions/1/images/train...\n",
      "Processed 1000 images from /Users/afifahhadi/.cache/kagglehub/datasets/jonathanoheix/face-expression-recognition-dataset/versions/1/images/train...\n",
      "Processed 1500 images from /Users/afifahhadi/.cache/kagglehub/datasets/jonathanoheix/face-expression-recognition-dataset/versions/1/images/train...\n",
      "Processed 2000 images from /Users/afifahhadi/.cache/kagglehub/datasets/jonathanoheix/face-expression-recognition-dataset/versions/1/images/train...\n",
      "Processed 2500 images from /Users/afifahhadi/.cache/kagglehub/datasets/jonathanoheix/face-expression-recognition-dataset/versions/1/images/train...\n",
      "Processed 3000 images from /Users/afifahhadi/.cache/kagglehub/datasets/jonathanoheix/face-expression-recognition-dataset/versions/1/images/train...\n",
      "Processed 3500 images from /Users/afifahhadi/.cache/kagglehub/datasets/jonathanoheix/face-expression-recognition-dataset/versions/1/images/train...\n",
      "Processed 4000 images from /Users/afifahhadi/.cache/kagglehub/datasets/jonathanoheix/face-expression-recognition-dataset/versions/1/images/train...\n",
      "Processed 4500 images from /Users/afifahhadi/.cache/kagglehub/datasets/jonathanoheix/face-expression-recognition-dataset/versions/1/images/train...\n",
      "Processed 5000 images from /Users/afifahhadi/.cache/kagglehub/datasets/jonathanoheix/face-expression-recognition-dataset/versions/1/images/train...\n",
      "Processed 5500 images from /Users/afifahhadi/.cache/kagglehub/datasets/jonathanoheix/face-expression-recognition-dataset/versions/1/images/train...\n",
      "Processed 6000 images from /Users/afifahhadi/.cache/kagglehub/datasets/jonathanoheix/face-expression-recognition-dataset/versions/1/images/train...\n",
      "Processed 6500 images from /Users/afifahhadi/.cache/kagglehub/datasets/jonathanoheix/face-expression-recognition-dataset/versions/1/images/train...\n",
      "Processed 7000 images from /Users/afifahhadi/.cache/kagglehub/datasets/jonathanoheix/face-expression-recognition-dataset/versions/1/images/train...\n",
      "Processed 7500 images from /Users/afifahhadi/.cache/kagglehub/datasets/jonathanoheix/face-expression-recognition-dataset/versions/1/images/train...\n",
      "Processed 8000 images from /Users/afifahhadi/.cache/kagglehub/datasets/jonathanoheix/face-expression-recognition-dataset/versions/1/images/train...\n",
      "Processed 8500 images from /Users/afifahhadi/.cache/kagglehub/datasets/jonathanoheix/face-expression-recognition-dataset/versions/1/images/train...\n",
      "Processed 9000 images from /Users/afifahhadi/.cache/kagglehub/datasets/jonathanoheix/face-expression-recognition-dataset/versions/1/images/train...\n",
      "Processed 9500 images from /Users/afifahhadi/.cache/kagglehub/datasets/jonathanoheix/face-expression-recognition-dataset/versions/1/images/train...\n",
      "Processed 10000 images from /Users/afifahhadi/.cache/kagglehub/datasets/jonathanoheix/face-expression-recognition-dataset/versions/1/images/train...\n",
      "Processed 10500 images from /Users/afifahhadi/.cache/kagglehub/datasets/jonathanoheix/face-expression-recognition-dataset/versions/1/images/train...\n",
      "Processed 11000 images from /Users/afifahhadi/.cache/kagglehub/datasets/jonathanoheix/face-expression-recognition-dataset/versions/1/images/train...\n",
      "Processed 11500 images from /Users/afifahhadi/.cache/kagglehub/datasets/jonathanoheix/face-expression-recognition-dataset/versions/1/images/train...\n",
      "Processed 12000 images from /Users/afifahhadi/.cache/kagglehub/datasets/jonathanoheix/face-expression-recognition-dataset/versions/1/images/train...\n",
      "Processed 12500 images from /Users/afifahhadi/.cache/kagglehub/datasets/jonathanoheix/face-expression-recognition-dataset/versions/1/images/train...\n",
      "Processed 13000 images from /Users/afifahhadi/.cache/kagglehub/datasets/jonathanoheix/face-expression-recognition-dataset/versions/1/images/train...\n",
      "Processed 13500 images from /Users/afifahhadi/.cache/kagglehub/datasets/jonathanoheix/face-expression-recognition-dataset/versions/1/images/train...\n",
      "Processed 14000 images from /Users/afifahhadi/.cache/kagglehub/datasets/jonathanoheix/face-expression-recognition-dataset/versions/1/images/train...\n",
      "Processed 14500 images from /Users/afifahhadi/.cache/kagglehub/datasets/jonathanoheix/face-expression-recognition-dataset/versions/1/images/train...\n",
      "Processed 15000 images from /Users/afifahhadi/.cache/kagglehub/datasets/jonathanoheix/face-expression-recognition-dataset/versions/1/images/train...\n",
      "Processed 15500 images from /Users/afifahhadi/.cache/kagglehub/datasets/jonathanoheix/face-expression-recognition-dataset/versions/1/images/train...\n",
      "Processed 16000 images from /Users/afifahhadi/.cache/kagglehub/datasets/jonathanoheix/face-expression-recognition-dataset/versions/1/images/train...\n",
      "Processed 16500 images from /Users/afifahhadi/.cache/kagglehub/datasets/jonathanoheix/face-expression-recognition-dataset/versions/1/images/train...\n",
      "Processed 17000 images from /Users/afifahhadi/.cache/kagglehub/datasets/jonathanoheix/face-expression-recognition-dataset/versions/1/images/train...\n",
      "Processed 17500 images from /Users/afifahhadi/.cache/kagglehub/datasets/jonathanoheix/face-expression-recognition-dataset/versions/1/images/train...\n",
      "Processed 18000 images from /Users/afifahhadi/.cache/kagglehub/datasets/jonathanoheix/face-expression-recognition-dataset/versions/1/images/train...\n",
      "Processed 18500 images from /Users/afifahhadi/.cache/kagglehub/datasets/jonathanoheix/face-expression-recognition-dataset/versions/1/images/train...\n",
      "Processed 19000 images from /Users/afifahhadi/.cache/kagglehub/datasets/jonathanoheix/face-expression-recognition-dataset/versions/1/images/train...\n",
      "Processed 19500 images from /Users/afifahhadi/.cache/kagglehub/datasets/jonathanoheix/face-expression-recognition-dataset/versions/1/images/train...\n",
      "Processed 20000 images from /Users/afifahhadi/.cache/kagglehub/datasets/jonathanoheix/face-expression-recognition-dataset/versions/1/images/train...\n",
      "Processed 20500 images from /Users/afifahhadi/.cache/kagglehub/datasets/jonathanoheix/face-expression-recognition-dataset/versions/1/images/train...\n",
      "Processed 21000 images from /Users/afifahhadi/.cache/kagglehub/datasets/jonathanoheix/face-expression-recognition-dataset/versions/1/images/train...\n",
      "Processed 21500 images from /Users/afifahhadi/.cache/kagglehub/datasets/jonathanoheix/face-expression-recognition-dataset/versions/1/images/train...\n",
      "Processed 22000 images from /Users/afifahhadi/.cache/kagglehub/datasets/jonathanoheix/face-expression-recognition-dataset/versions/1/images/train...\n",
      "Processed 22500 images from /Users/afifahhadi/.cache/kagglehub/datasets/jonathanoheix/face-expression-recognition-dataset/versions/1/images/train...\n",
      "Processed 23000 images from /Users/afifahhadi/.cache/kagglehub/datasets/jonathanoheix/face-expression-recognition-dataset/versions/1/images/train...\n",
      "Processed 23500 images from /Users/afifahhadi/.cache/kagglehub/datasets/jonathanoheix/face-expression-recognition-dataset/versions/1/images/train...\n",
      "Processed 24000 images from /Users/afifahhadi/.cache/kagglehub/datasets/jonathanoheix/face-expression-recognition-dataset/versions/1/images/train...\n",
      "Processed 24500 images from /Users/afifahhadi/.cache/kagglehub/datasets/jonathanoheix/face-expression-recognition-dataset/versions/1/images/train...\n",
      "Processed 25000 images from /Users/afifahhadi/.cache/kagglehub/datasets/jonathanoheix/face-expression-recognition-dataset/versions/1/images/train...\n",
      "Processed 25500 images from /Users/afifahhadi/.cache/kagglehub/datasets/jonathanoheix/face-expression-recognition-dataset/versions/1/images/train...\n",
      "Processed 26000 images from /Users/afifahhadi/.cache/kagglehub/datasets/jonathanoheix/face-expression-recognition-dataset/versions/1/images/train...\n",
      "Processed 26500 images from /Users/afifahhadi/.cache/kagglehub/datasets/jonathanoheix/face-expression-recognition-dataset/versions/1/images/train...\n",
      "Writing 26784 rows to ../data/train_features.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1764916293.252725 3565066 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 90.5), renderer: Apple M1\n",
      "W0000 00:00:1764916293.269102 3573408 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1764916293.276985 3573408 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 500 images from /Users/afifahhadi/.cache/kagglehub/datasets/jonathanoheix/face-expression-recognition-dataset/versions/1/images/validation...\n",
      "Processed 1000 images from /Users/afifahhadi/.cache/kagglehub/datasets/jonathanoheix/face-expression-recognition-dataset/versions/1/images/validation...\n",
      "Processed 1500 images from /Users/afifahhadi/.cache/kagglehub/datasets/jonathanoheix/face-expression-recognition-dataset/versions/1/images/validation...\n",
      "Processed 2000 images from /Users/afifahhadi/.cache/kagglehub/datasets/jonathanoheix/face-expression-recognition-dataset/versions/1/images/validation...\n",
      "Processed 2500 images from /Users/afifahhadi/.cache/kagglehub/datasets/jonathanoheix/face-expression-recognition-dataset/versions/1/images/validation...\n",
      "Processed 3000 images from /Users/afifahhadi/.cache/kagglehub/datasets/jonathanoheix/face-expression-recognition-dataset/versions/1/images/validation...\n",
      "Processed 3500 images from /Users/afifahhadi/.cache/kagglehub/datasets/jonathanoheix/face-expression-recognition-dataset/versions/1/images/validation...\n",
      "Processed 4000 images from /Users/afifahhadi/.cache/kagglehub/datasets/jonathanoheix/face-expression-recognition-dataset/versions/1/images/validation...\n",
      "Processed 4500 images from /Users/afifahhadi/.cache/kagglehub/datasets/jonathanoheix/face-expression-recognition-dataset/versions/1/images/validation...\n",
      "Processed 5000 images from /Users/afifahhadi/.cache/kagglehub/datasets/jonathanoheix/face-expression-recognition-dataset/versions/1/images/validation...\n",
      "Processed 5500 images from /Users/afifahhadi/.cache/kagglehub/datasets/jonathanoheix/face-expression-recognition-dataset/versions/1/images/validation...\n",
      "Processed 6000 images from /Users/afifahhadi/.cache/kagglehub/datasets/jonathanoheix/face-expression-recognition-dataset/versions/1/images/validation...\n",
      "Processed 6500 images from /Users/afifahhadi/.cache/kagglehub/datasets/jonathanoheix/face-expression-recognition-dataset/versions/1/images/validation...\n",
      "Writing 6599 rows to ../data/val_features.csv\n"
     ]
    }
   ],
   "source": [
    "train_dir, val_dir = get_dataset_paths()\n",
    "preprocess_to_csv(train_dir, \"../data/train_features.csv\")\n",
    "preprocess_to_csv(val_dir, \"../data/val_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ecdff1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv(\"../data/train_features.csv\")\n",
    "val_df = pd.read_csv(\"../data/val_features.csv\")\n",
    "\n",
    "X_train = train_df.drop(\"label\", axis=1).values.astype(np.float32)\n",
    "y_train = train_df[\"label\"].values\n",
    "\n",
    "X_val = val_df.drop(\"label\", axis=1).values.astype(np.float32)\n",
    "y_val = val_df[\"label\"].values\n",
    "\n",
    "# Save as .npy for easy loading\n",
    "os.makedirs(\"../data\", exist_ok=True)\n",
    "np.save(\"../data/X_train.npy\", X_train)\n",
    "np.save(\"../data/y_train.npy\", y_train)\n",
    "np.save(\"../data/X_val.npy\", X_val)\n",
    "np.save(\"../data/y_val.npy\", y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43b89e19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['mouth_width', 'mouth_height', 'mouth_ar', 'mouth_corner_asym_norm',\n",
       "       'mouth_center_nose_dist', 'mouth_slope_left', 'mouth_slope_right',\n",
       "       'mouth_slope_mean', 'mouth_slope_diff', 'lip_thickness_mid',\n",
       "       'mouth_corner_top_left', 'mouth_corner_top_right',\n",
       "       'mouth_corner_bottom_left', 'mouth_corner_bottom_right',\n",
       "       'mouth_corner_nose_left', 'mouth_corner_nose_right',\n",
       "       'upper_lip_curvature', 'lower_lip_curvature', 'left_eye_width',\n",
       "       'left_eye_height', 'right_eye_width', 'right_eye_height', 'left_ear',\n",
       "       'right_ear', 'mean_ear', 'ear_diff', 'left_cheek_eye',\n",
       "       'right_cheek_eye', 'cheek_eye_mean', 'cheek_eye_diff', 'left_brow_eye',\n",
       "       'right_brow_eye', 'brow_eye_mean', 'brow_eye_diff', 'inner_brow_dist',\n",
       "       'left_brow_tilt', 'right_brow_tilt', 'brow_tilt_diff', 'left_brow_curv',\n",
       "       'right_brow_curv', 'brow_curv_mean', 'brow_curv_diff',\n",
       "       'left_inner_brow_eye', 'left_outer_brow_eye', 'right_inner_brow_eye',\n",
       "       'right_outer_brow_eye', 'inner_brow_eye_mean', 'outer_brow_eye_mean',\n",
       "       'inner_outer_brow_eye_diff', 'face_aspect', 'head_tilt_angle',\n",
       "       'head_tilt_sin', 'head_tilt_cos', 'label'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25b3cfe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "happy       6899\n",
       "neutral     4819\n",
       "sad         4409\n",
       "fear        3699\n",
       "angry       3544\n",
       "surprise    3036\n",
       "disgust      378\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d71647a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f0</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>...</th>\n",
       "      <th>f14</th>\n",
       "      <th>f15</th>\n",
       "      <th>f16</th>\n",
       "      <th>f17</th>\n",
       "      <th>f18</th>\n",
       "      <th>f19</th>\n",
       "      <th>f20</th>\n",
       "      <th>f21</th>\n",
       "      <th>f22</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3544</th>\n",
       "      <td>0.413178</td>\n",
       "      <td>0.192618</td>\n",
       "      <td>0.466185</td>\n",
       "      <td>-0.057908</td>\n",
       "      <td>0.333224</td>\n",
       "      <td>0.164280</td>\n",
       "      <td>0.164278</td>\n",
       "      <td>0.164279</td>\n",
       "      <td>2.021374e-06</td>\n",
       "      <td>0.188651</td>\n",
       "      <td>...</td>\n",
       "      <td>0.197444</td>\n",
       "      <td>0.225921</td>\n",
       "      <td>0.056953</td>\n",
       "      <td>-0.147598</td>\n",
       "      <td>-0.130699</td>\n",
       "      <td>-0.139149</td>\n",
       "      <td>-0.016900</td>\n",
       "      <td>1.155492</td>\n",
       "      <td>0.148629</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3545</th>\n",
       "      <td>0.366580</td>\n",
       "      <td>0.002975</td>\n",
       "      <td>0.008116</td>\n",
       "      <td>-0.005619</td>\n",
       "      <td>0.274076</td>\n",
       "      <td>0.018910</td>\n",
       "      <td>0.018910</td>\n",
       "      <td>0.018910</td>\n",
       "      <td>2.628474e-07</td>\n",
       "      <td>0.229015</td>\n",
       "      <td>...</td>\n",
       "      <td>0.332040</td>\n",
       "      <td>0.316955</td>\n",
       "      <td>-0.030171</td>\n",
       "      <td>-0.131915</td>\n",
       "      <td>-0.099154</td>\n",
       "      <td>-0.115535</td>\n",
       "      <td>-0.032760</td>\n",
       "      <td>1.179423</td>\n",
       "      <td>-0.123804</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3546</th>\n",
       "      <td>0.368501</td>\n",
       "      <td>0.018032</td>\n",
       "      <td>0.048933</td>\n",
       "      <td>0.000255</td>\n",
       "      <td>0.268026</td>\n",
       "      <td>-0.000843</td>\n",
       "      <td>-0.000843</td>\n",
       "      <td>-0.000843</td>\n",
       "      <td>-1.107547e-08</td>\n",
       "      <td>0.197324</td>\n",
       "      <td>...</td>\n",
       "      <td>0.315069</td>\n",
       "      <td>0.308115</td>\n",
       "      <td>-0.013909</td>\n",
       "      <td>-0.071679</td>\n",
       "      <td>-0.119965</td>\n",
       "      <td>-0.095822</td>\n",
       "      <td>0.048286</td>\n",
       "      <td>1.209658</td>\n",
       "      <td>-0.038514</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3547</th>\n",
       "      <td>0.352277</td>\n",
       "      <td>0.082331</td>\n",
       "      <td>0.233710</td>\n",
       "      <td>0.023231</td>\n",
       "      <td>0.277251</td>\n",
       "      <td>-0.084161</td>\n",
       "      <td>-0.084159</td>\n",
       "      <td>-0.084160</td>\n",
       "      <td>-1.246755e-06</td>\n",
       "      <td>0.205144</td>\n",
       "      <td>...</td>\n",
       "      <td>0.320739</td>\n",
       "      <td>0.301902</td>\n",
       "      <td>-0.037674</td>\n",
       "      <td>-0.095715</td>\n",
       "      <td>-0.093208</td>\n",
       "      <td>-0.094462</td>\n",
       "      <td>-0.002507</td>\n",
       "      <td>1.252114</td>\n",
       "      <td>-0.079918</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3548</th>\n",
       "      <td>0.347480</td>\n",
       "      <td>0.075914</td>\n",
       "      <td>0.218471</td>\n",
       "      <td>0.049908</td>\n",
       "      <td>0.283811</td>\n",
       "      <td>-0.179168</td>\n",
       "      <td>-0.179166</td>\n",
       "      <td>-0.179167</td>\n",
       "      <td>-2.543680e-06</td>\n",
       "      <td>0.201779</td>\n",
       "      <td>...</td>\n",
       "      <td>0.318796</td>\n",
       "      <td>0.324344</td>\n",
       "      <td>0.011097</td>\n",
       "      <td>-0.089835</td>\n",
       "      <td>-0.064179</td>\n",
       "      <td>-0.077007</td>\n",
       "      <td>-0.025655</td>\n",
       "      <td>1.150373</td>\n",
       "      <td>-0.201656</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3917</th>\n",
       "      <td>0.450030</td>\n",
       "      <td>0.134156</td>\n",
       "      <td>0.298103</td>\n",
       "      <td>-0.017895</td>\n",
       "      <td>0.296697</td>\n",
       "      <td>0.054205</td>\n",
       "      <td>0.054205</td>\n",
       "      <td>0.054205</td>\n",
       "      <td>5.784295e-07</td>\n",
       "      <td>0.212070</td>\n",
       "      <td>...</td>\n",
       "      <td>0.300434</td>\n",
       "      <td>0.260787</td>\n",
       "      <td>-0.079293</td>\n",
       "      <td>-0.080248</td>\n",
       "      <td>-0.075953</td>\n",
       "      <td>-0.078101</td>\n",
       "      <td>-0.004294</td>\n",
       "      <td>1.332551</td>\n",
       "      <td>0.028317</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3918</th>\n",
       "      <td>0.450030</td>\n",
       "      <td>0.134156</td>\n",
       "      <td>0.298103</td>\n",
       "      <td>-0.017895</td>\n",
       "      <td>0.296697</td>\n",
       "      <td>0.054205</td>\n",
       "      <td>0.054205</td>\n",
       "      <td>0.054205</td>\n",
       "      <td>5.784295e-07</td>\n",
       "      <td>0.212070</td>\n",
       "      <td>...</td>\n",
       "      <td>0.300434</td>\n",
       "      <td>0.260787</td>\n",
       "      <td>-0.079293</td>\n",
       "      <td>-0.080248</td>\n",
       "      <td>-0.075953</td>\n",
       "      <td>-0.078101</td>\n",
       "      <td>-0.004294</td>\n",
       "      <td>1.332551</td>\n",
       "      <td>0.028317</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3919</th>\n",
       "      <td>0.284852</td>\n",
       "      <td>0.016211</td>\n",
       "      <td>0.056910</td>\n",
       "      <td>-0.001232</td>\n",
       "      <td>0.307201</td>\n",
       "      <td>0.004811</td>\n",
       "      <td>0.004811</td>\n",
       "      <td>0.004811</td>\n",
       "      <td>7.919468e-08</td>\n",
       "      <td>0.203227</td>\n",
       "      <td>...</td>\n",
       "      <td>0.250468</td>\n",
       "      <td>0.295314</td>\n",
       "      <td>0.089692</td>\n",
       "      <td>-0.111745</td>\n",
       "      <td>-0.098683</td>\n",
       "      <td>-0.105214</td>\n",
       "      <td>-0.013062</td>\n",
       "      <td>1.076356</td>\n",
       "      <td>0.012680</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3920</th>\n",
       "      <td>0.419064</td>\n",
       "      <td>0.104245</td>\n",
       "      <td>0.248757</td>\n",
       "      <td>-0.018916</td>\n",
       "      <td>0.307016</td>\n",
       "      <td>0.053849</td>\n",
       "      <td>0.053848</td>\n",
       "      <td>0.053848</td>\n",
       "      <td>5.752945e-07</td>\n",
       "      <td>0.194829</td>\n",
       "      <td>...</td>\n",
       "      <td>0.333145</td>\n",
       "      <td>0.346356</td>\n",
       "      <td>0.026423</td>\n",
       "      <td>-0.082563</td>\n",
       "      <td>-0.085388</td>\n",
       "      <td>-0.083975</td>\n",
       "      <td>0.002825</td>\n",
       "      <td>1.176775</td>\n",
       "      <td>-0.009330</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3921</th>\n",
       "      <td>0.324195</td>\n",
       "      <td>0.008361</td>\n",
       "      <td>0.025791</td>\n",
       "      <td>0.000654</td>\n",
       "      <td>0.308482</td>\n",
       "      <td>-0.002695</td>\n",
       "      <td>-0.002695</td>\n",
       "      <td>-0.002695</td>\n",
       "      <td>-4.661496e-08</td>\n",
       "      <td>0.173735</td>\n",
       "      <td>...</td>\n",
       "      <td>0.184641</td>\n",
       "      <td>0.234363</td>\n",
       "      <td>0.099444</td>\n",
       "      <td>-0.072828</td>\n",
       "      <td>-0.091050</td>\n",
       "      <td>-0.081939</td>\n",
       "      <td>0.018222</td>\n",
       "      <td>1.164197</td>\n",
       "      <td>-0.043319</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>378 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            f0        f1        f2        f3        f4        f5        f6  \\\n",
       "3544  0.413178  0.192618  0.466185 -0.057908  0.333224  0.164280  0.164278   \n",
       "3545  0.366580  0.002975  0.008116 -0.005619  0.274076  0.018910  0.018910   \n",
       "3546  0.368501  0.018032  0.048933  0.000255  0.268026 -0.000843 -0.000843   \n",
       "3547  0.352277  0.082331  0.233710  0.023231  0.277251 -0.084161 -0.084159   \n",
       "3548  0.347480  0.075914  0.218471  0.049908  0.283811 -0.179168 -0.179166   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "3917  0.450030  0.134156  0.298103 -0.017895  0.296697  0.054205  0.054205   \n",
       "3918  0.450030  0.134156  0.298103 -0.017895  0.296697  0.054205  0.054205   \n",
       "3919  0.284852  0.016211  0.056910 -0.001232  0.307201  0.004811  0.004811   \n",
       "3920  0.419064  0.104245  0.248757 -0.018916  0.307016  0.053849  0.053848   \n",
       "3921  0.324195  0.008361  0.025791  0.000654  0.308482 -0.002695 -0.002695   \n",
       "\n",
       "            f7            f8        f9  ...       f14       f15       f16  \\\n",
       "3544  0.164279  2.021374e-06  0.188651  ...  0.197444  0.225921  0.056953   \n",
       "3545  0.018910  2.628474e-07  0.229015  ...  0.332040  0.316955 -0.030171   \n",
       "3546 -0.000843 -1.107547e-08  0.197324  ...  0.315069  0.308115 -0.013909   \n",
       "3547 -0.084160 -1.246755e-06  0.205144  ...  0.320739  0.301902 -0.037674   \n",
       "3548 -0.179167 -2.543680e-06  0.201779  ...  0.318796  0.324344  0.011097   \n",
       "...        ...           ...       ...  ...       ...       ...       ...   \n",
       "3917  0.054205  5.784295e-07  0.212070  ...  0.300434  0.260787 -0.079293   \n",
       "3918  0.054205  5.784295e-07  0.212070  ...  0.300434  0.260787 -0.079293   \n",
       "3919  0.004811  7.919468e-08  0.203227  ...  0.250468  0.295314  0.089692   \n",
       "3920  0.053848  5.752945e-07  0.194829  ...  0.333145  0.346356  0.026423   \n",
       "3921 -0.002695 -4.661496e-08  0.173735  ...  0.184641  0.234363  0.099444   \n",
       "\n",
       "           f17       f18       f19       f20       f21       f22    label  \n",
       "3544 -0.147598 -0.130699 -0.139149 -0.016900  1.155492  0.148629  disgust  \n",
       "3545 -0.131915 -0.099154 -0.115535 -0.032760  1.179423 -0.123804  disgust  \n",
       "3546 -0.071679 -0.119965 -0.095822  0.048286  1.209658 -0.038514  disgust  \n",
       "3547 -0.095715 -0.093208 -0.094462 -0.002507  1.252114 -0.079918  disgust  \n",
       "3548 -0.089835 -0.064179 -0.077007 -0.025655  1.150373 -0.201656  disgust  \n",
       "...        ...       ...       ...       ...       ...       ...      ...  \n",
       "3917 -0.080248 -0.075953 -0.078101 -0.004294  1.332551  0.028317  disgust  \n",
       "3918 -0.080248 -0.075953 -0.078101 -0.004294  1.332551  0.028317  disgust  \n",
       "3919 -0.111745 -0.098683 -0.105214 -0.013062  1.076356  0.012680  disgust  \n",
       "3920 -0.082563 -0.085388 -0.083975  0.002825  1.176775 -0.009330  disgust  \n",
       "3921 -0.072828 -0.091050 -0.081939  0.018222  1.164197 -0.043319  disgust  \n",
       "\n",
       "[378 rows x 24 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[train_df[\"label\"] == \"disgust\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae4d510",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
